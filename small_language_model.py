# -*- coding: utf-8 -*-
"""Small_Language_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z8XKI69_1NkMih2TF6dG_xLYSdpuOz0O
"""

!pip install torch torchtext transformers sentencepiece tqdm datasets pandas

import pandas as pd
import ast
from datasets import Dataset,DatasetDict,load_dataset
from tqdm import tqdm
import datasets

#load the dataset
data_sample = load_dataset("QuyenAnhDE/Diseases_Symptoms")

data_sample

updated_data = [{'Name' : item['Name'],'Symptoms':item['Symptoms']} for item in data_sample['train']]

import pandas as pd
df = pd.DataFrame(updated_data)
df.head(5)

df['Symptoms'] = df['Symptoms'].apply(lambda x: ','.join(x.split(', ')))

from transformers import GPT2Tokenizer, GPT2LMHeadModel
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split

if torch.cuda.is_available():
    device = torch.device("cuda")
else:
    device = torch.device("cpu")

device

tokenizer = GPT2Tokenizer.from_pretrained("distilbert/distilgpt2")
model = GPT2LMHeadModel.from_pretrained("distilbert/distilgpt2").to(device)

model

BATCH_SIZE = 8

df.describe()

#Dataset Preperation
class Languagedataset(Dataset):
  def __init__(self,df,tokenizer):
    self.df = df
    self.labels = df.columns
    self.data = df.to_dict(orient='records')
    self.tokenizer = tokenizer
    x = self.fittest_max_length(df)
    self.max_length = x
#Length of the dataset
  def __len__(self):
    return len(self.data)
#Getting one trainning sample
  def __getitem__(self, idx):
    x = self.data[idx][self.labels[0]]
    y = self.data[idx][self.labels[1]]
    text = f"{x} | {y}"
    tokens = self.tokenizer(  # ✅ Use __call__ method
        text,
        return_tensors='pt',  # ✅ Also fixed typo: return_tensors not return_tensor
        max_length=128,
        padding='max_length',
        truncation=True
    )
    return tokens

  def fittest_max_length(self,df):
    max_length = max(len(max(df[self.labels[0]],key = len)),
                     len(max(df[self.labels[1]],key = len))
                     )
    x = 2
    while x < max_length:x = x*2
    return x

data_sample = Languagedataset(df,tokenizer)

data_sample

train_size = int(0.8* len(data_sample))
val_size = len(data_sample) - train_size
train_data,val_data = random_split(data_sample,[train_size,val_size])

#Make the iterators
train_loader = DataLoader(train_data,batch_size=BATCH_SIZE,shuffle=True)
val_loader = DataLoader(val_data,batch_size=BATCH_SIZE)

num_epochs = 10

batch_size = BATCH_SIZE
model_name = "distilgpt2"
gpu = 0

#Loss function and Optimizer
criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_type_id)
optimizer = optim.Adam(model.parameters(),lr=5e-4)
tokenizer.pad_token = tokenizer.eos_token

results = pd.DataFrame(columns=['epoch','transfomer','batch_size','gpu','trainning_loss','val_loss','epoch_duration_sec'])

import time

for epoch in range(num_epochs):
    start_time = time.time()

    # ================= TRAIN =================
    model.train()
    epoch_training_loss = 0

    train_iterator = tqdm(
        train_loader,
        desc=f"Training epoch {epoch+1}/{num_epochs}"
    )

    for batch in train_iterator:
        optimizer.zero_grad()

        inputs = batch['input_ids'].to(device)
        if inputs.dim() == 3:
            inputs = inputs.squeeze(1)

        outputs = model(input_ids=inputs, labels=inputs)
        loss = outputs.loss

        loss.backward()
        optimizer.step()

        epoch_training_loss += loss.item()
        train_iterator.set_postfix(loss=loss.item())

    avg_epoch_training_loss = epoch_training_loss / len(train_loader)

    # ================= VALIDATION =================
    model.eval()
    epoch_val_loss = 0

    with torch.no_grad():
        valid_iterator = tqdm(val_loader, desc=f"Validating epoch {epoch+1}/{num_epochs}")

        for batch in valid_iterator:
            inputs = batch['input_ids'].to(device)
            if inputs.dim() == 3:
                inputs = inputs.squeeze(1)

            outputs = model(input_ids=inputs, labels=inputs)
            loss = outputs.loss

            epoch_val_loss += loss.item()
            valid_iterator.set_postfix(loss=loss.item())

    avg_epoch_val_loss = epoch_val_loss / len(val_loader)

    epoch_duration = time.time() - start_time

    results.loc[len(results)] = {
        'epoch': epoch+1,
        'transformer': model_name,
        'batch_size': batch_size,
        'gpu': gpu,
        'trainning_loss': avg_epoch_training_loss,
        'val_loss': avg_epoch_val_loss,
        'epoch_duration_sec': epoch_duration
    }

    print(
        f"Epoch {epoch+1} | "
        f"Train Loss: {avg_epoch_training_loss:.4f} | "
        f"Val Loss: {avg_epoch_val_loss:.4f} | "
        f"Time: {epoch_duration:.2f}s"
    )

input_str = "Depression"

input_ids = tokenizer.encode(input_str, return_tensors='pt').to(device)

input_ids

output = model.generate(input_ids, max_length=20, num_return_sequences=1, do_sample = True,top_k = 8,top_p = 0.95,temperature = 0.5)

output

decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)

decoded_output

torch.save(model,"SmallDiseasesLM.pt")

torch.save(model,'drive/My Drive/SmallDiseasesLM.pt')